{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xid_FLxKdC6L"
      },
      "source": [
        "# SVM Vs ResNet DS675 Project\n",
        "This project makes use of the CIFAR-10 Image classification datset to compare the performance of SVM and CNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6gHYfcU_7gQK"
      },
      "outputs": [],
      "source": [
        "# accessing the data batches and combining to from train and test set\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QruyGeHfdsSZ"
      },
      "outputs": [],
      "source": [
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "for i in range(1, 6):\n",
        "    batch = unpickle(f'cifar-10-batches-py/data_batch_{i}') # batches are of shape (10000, 3072)\n",
        "    X_train.append(batch[b'data'])\n",
        "    y_train.extend(batch[b'labels'])\n",
        "\n",
        "X_train = np.vstack(X_train)   # shape (50000, 3072), after vertically stacking\n",
        "y_train = np.array(y_train)    # shape (50000, )\n",
        "\n",
        "# Test set\n",
        "test_batch = unpickle('cifar-10-batches-py/test_batch')\n",
        "X_test = test_batch[b'data']   # shape (10000, 3072)\n",
        "y_test = np.array(test_batch[b'labels'])  #shape (10000, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxSWu0-vevmw"
      },
      "source": [
        "The SVM uses flat image data(1D) which is what the current X_train and X_test shapes are, but NN requires it to be in the true image format of 32 x 32 i.e 2D (3 channel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "uqut8t90evPf"
      },
      "outputs": [],
      "source": [
        "# reshape the image data to be compliant with CNN\n",
        "X_train_img = X_train.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
        "X_test_img = X_test.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOFVuJXZvtlp"
      },
      "source": [
        "# Preprocessing for each model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DKKSrD0zfZvy"
      },
      "outputs": [],
      "source": [
        "# SVM - Standardize\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_svm = scaler.fit_transform(X_train.astype(\"float32\"))\n",
        "X_test_svm = scaler.transform(X_test.astype(\"float32\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rVGopX8vxE0R"
      },
      "outputs": [],
      "source": [
        "# CNN - Normalize\n",
        "X_train_cnn = X_train_img.astype('float32') / 255.0\n",
        "X_test_cnn = X_test_img.astype('float32') / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwTQTIiLxJ07",
        "outputId": "156c02ac-c6a6-4ffd-b773-2e8302793527"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(50000, 3072) (10000, 3072)\n",
            "(50000, 32, 32, 3) (10000, 32, 32, 3)\n",
            "(50000,) (10000,)\n",
            "uint8 uint8\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape, X_test.shape)\n",
        "print(X_train_img.shape, X_test_img.shape)\n",
        "print(y_train.shape, y_test.shape)\n",
        "print(X_train.dtype, X_train_img.dtype)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ-Fto2J1Muu"
      },
      "source": [
        "# Moving onto SVM 5 fold CV + grid on C, record runtime + inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6tZk2eVb1S54"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "import time, pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "seed = 42\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "\n",
        "param_grid = {'C':[0.1, 0.01, 0.001]}\n",
        "\n",
        "clf = LinearSVC(dual=False, max_iter=10000, random_state=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHXsElUk1qVB",
        "outputId": "6434f745-8230-4ce8-97ab-a9afd2d4821b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
            "GridSearch total time: 21674.4 s\n"
          ]
        }
      ],
      "source": [
        "grid = GridSearchCV(clf, param_grid, cv=cv, scoring='accuracy', return_train_score=True, n_jobs=2, verbose=2)\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "grid.fit(X_train_svm, y_train)\n",
        "t1 = time.perf_counter()\n",
        "print(f\"GridSearch total time: {t1-t0:.1f} s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ejuSnp4v1wFw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         params  mean_test_score  std_test_score  mean_fit_time  \\\n",
            "0    {'C': 0.1}          0.36012        0.002851    4792.973716   \n",
            "1   {'C': 0.01}          0.37592        0.005380    2044.011279   \n",
            "2  {'C': 0.001}          0.39440        0.004984     944.685522   \n",
            "\n",
            "   mean_score_time  \n",
            "0         0.178174  \n",
            "1         0.128166  \n",
            "2         0.108715  \n",
            "Best params: {'C': 0.001} CV acc: 0.39440000000000003\n"
          ]
        }
      ],
      "source": [
        "# summary results\n",
        "res = pd.DataFrame(grid.cv_results_)\n",
        "print(res[['params','mean_test_score','std_test_score','mean_fit_time','mean_score_time']])\n",
        "\n",
        "best = grid.best_estimator_\n",
        "print(\"Best params:\", grid.best_params_, \"CV acc:\", grid.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9LX8iv612Imn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.3975\n",
            "Inference time (avg ms/sample): 0.034849630000826436\n",
            "Confusion matrix:\n",
            " [[496  49  34  21  24  21  27  53 194  81]\n",
            " [ 68 490  18  21  18  29  44  58  78 176]\n",
            " [110  52 227  69 104  88 168  90  62  30]\n",
            " [ 63  74  79 179  44 196 178  52  57  78]\n",
            " [ 64  39 105  51 261  89 194 125  32  40]\n",
            " [ 50  70  78 122  73 324 104  78  58  43]\n",
            " [ 26  57  62  83  84  73 512  41  24  38]\n",
            " [ 62  58  52  39  69  78  50 452  44  96]\n",
            " [135  77  12  18   7  36  19  22 563 111]\n",
            " [ 62 183  18  21  16  25  54  59  91 471]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.50      0.46      1000\n",
            "           1       0.43      0.49      0.46      1000\n",
            "           2       0.33      0.23      0.27      1000\n",
            "           3       0.29      0.18      0.22      1000\n",
            "           4       0.37      0.26      0.31      1000\n",
            "           5       0.34      0.32      0.33      1000\n",
            "           6       0.38      0.51      0.44      1000\n",
            "           7       0.44      0.45      0.45      1000\n",
            "           8       0.47      0.56      0.51      1000\n",
            "           9       0.40      0.47      0.44      1000\n",
            "\n",
            "    accuracy                           0.40     10000\n",
            "   macro avg       0.39      0.40      0.39     10000\n",
            "weighted avg       0.39      0.40      0.39     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Inference time on final test set\n",
        "t0 = time.perf_counter()\n",
        "y_pred_test = best.predict(X_test_svm)\n",
        "t1 = time.perf_counter()\n",
        "inf_time_per_sample = (t1-t0)/len(X_test_svm)\n",
        "print(\"Test accuracy:\", accuracy_score(y_test, y_pred_test))\n",
        "print(\"Inference time (avg ms/sample):\", inf_time_per_sample*1000)\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
        "print(classification_report(y_test, y_pred_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Moving onto ResNet18 for CIFAR10 dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# using pytorch for ResNet18\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet18\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# converting the previously NHWC format(batch, height, width, channels) to NCHW format (batch, channels, height, width)\n",
        "X_train_tensor = torch.tensor(X_train_cnn).permute(0, 3, 1, 2)  # (N, 3, 32, 32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test_cnn).permute(0, 3, 1, 2)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# using resnet without pretrained weights, i.e training from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def train_resnet(train_loader, val_loader=None, num_epochs=10, lr=0.001):\n",
        "    model = resnet18(weights=None)\n",
        "    model.fc = nn.Linear(model.fc.in_features, 10)\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    best_val_acc = 0\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0, 0, 0\n",
        "\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = preds.max(1)\n",
        "            total += yb.size(0)\n",
        "            correct += (predicted == yb).sum().item()\n",
        "\n",
        "        train_acc = correct / total\n",
        "\n",
        "        if val_loader:\n",
        "            model.eval()\n",
        "            correct, total = 0, 0\n",
        "            with torch.no_grad():\n",
        "                for xb, yb in val_loader:\n",
        "                    xb, yb = xb.to(device), yb.to(device)\n",
        "                    preds = model(xb)\n",
        "                    _, predicted = preds.max(1)\n",
        "                    total += yb.size(0)\n",
        "                    correct += (predicted == yb).sum().item()\n",
        "            val_acc = correct / total\n",
        "            print(f\"Epoch {epoch+1}: \"\n",
        "                  f\"Train Loss={running_loss/len(train_loader):.4f}, \"\n",
        "                  f\"Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}\")\n",
        "            best_val_acc = max(best_val_acc, val_acc)\n",
        "\n",
        "    total_time = time.perf_counter() - start_time\n",
        "    return best_val_acc, total_time, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Fold 1 =====\n",
            "Epoch 1: Train Loss=1.4404, Train Acc=0.4830, Val Acc=0.5310\n",
            "Epoch 2: Train Loss=1.0581, Train Acc=0.6264, Val Acc=0.5467\n",
            "Epoch 3: Train Loss=0.8890, Train Acc=0.6901, Val Acc=0.6265\n",
            "Epoch 4: Train Loss=0.7574, Train Acc=0.7367, Val Acc=0.6625\n",
            "Epoch 5: Train Loss=0.6483, Train Acc=0.7722, Val Acc=0.6769\n",
            "Epoch 6: Train Loss=0.5586, Train Acc=0.8052, Val Acc=0.6820\n",
            "Epoch 7: Train Loss=0.4718, Train Acc=0.8357, Val Acc=0.6808\n",
            "Epoch 8: Train Loss=0.3880, Train Acc=0.8652, Val Acc=0.7426\n",
            "Epoch 9: Train Loss=0.3144, Train Acc=0.8917, Val Acc=0.7340\n",
            "Epoch 10: Train Loss=0.2592, Train Acc=0.9093, Val Acc=0.7323\n",
            "\n",
            "===== Fold 2 =====\n",
            "Epoch 1: Train Loss=1.4513, Train Acc=0.4784, Val Acc=0.5086\n",
            "Epoch 2: Train Loss=1.0602, Train Acc=0.6282, Val Acc=0.6251\n",
            "Epoch 3: Train Loss=0.8797, Train Acc=0.6913, Val Acc=0.6413\n",
            "Epoch 4: Train Loss=0.7510, Train Acc=0.7371, Val Acc=0.6713\n",
            "Epoch 5: Train Loss=0.6467, Train Acc=0.7750, Val Acc=0.6899\n",
            "Epoch 6: Train Loss=0.5536, Train Acc=0.8079, Val Acc=0.7018\n",
            "Epoch 7: Train Loss=0.4613, Train Acc=0.8401, Val Acc=0.6697\n",
            "Epoch 8: Train Loss=0.3822, Train Acc=0.8665, Val Acc=0.7372\n",
            "Epoch 9: Train Loss=0.3140, Train Acc=0.8898, Val Acc=0.7362\n",
            "Epoch 10: Train Loss=0.2573, Train Acc=0.9096, Val Acc=0.7376\n",
            "\n",
            "===== Fold 3 =====\n",
            "Epoch 1: Train Loss=1.4395, Train Acc=0.4835, Val Acc=0.5667\n",
            "Epoch 2: Train Loss=1.0512, Train Acc=0.6280, Val Acc=0.4616\n",
            "Epoch 3: Train Loss=0.8663, Train Acc=0.6967, Val Acc=0.6162\n",
            "Epoch 4: Train Loss=0.7429, Train Acc=0.7414, Val Acc=0.7080\n",
            "Epoch 5: Train Loss=0.6364, Train Acc=0.7781, Val Acc=0.6617\n",
            "Epoch 6: Train Loss=0.5376, Train Acc=0.8128, Val Acc=0.6965\n",
            "Epoch 7: Train Loss=0.4508, Train Acc=0.8440, Val Acc=0.7416\n",
            "Epoch 8: Train Loss=0.3715, Train Acc=0.8696, Val Acc=0.7173\n",
            "Epoch 9: Train Loss=0.3030, Train Acc=0.8940, Val Acc=0.7324\n",
            "Epoch 10: Train Loss=0.2437, Train Acc=0.9146, Val Acc=0.7363\n",
            "\n",
            "===== Fold 4 =====\n",
            "Epoch 1: Train Loss=1.4463, Train Acc=0.4803, Val Acc=0.4850\n",
            "Epoch 2: Train Loss=1.0598, Train Acc=0.6246, Val Acc=0.5513\n",
            "Epoch 3: Train Loss=0.8752, Train Acc=0.6896, Val Acc=0.6543\n",
            "Epoch 4: Train Loss=0.7516, Train Acc=0.7354, Val Acc=0.6042\n",
            "Epoch 5: Train Loss=0.6374, Train Acc=0.7781, Val Acc=0.7149\n",
            "Epoch 6: Train Loss=0.5503, Train Acc=0.8084, Val Acc=0.7044\n",
            "Epoch 7: Train Loss=0.4595, Train Acc=0.8382, Val Acc=0.7081\n",
            "Epoch 8: Train Loss=0.3739, Train Acc=0.8687, Val Acc=0.6808\n",
            "Epoch 9: Train Loss=0.3105, Train Acc=0.8900, Val Acc=0.7376\n",
            "Epoch 10: Train Loss=0.2477, Train Acc=0.9122, Val Acc=0.7508\n",
            "\n",
            "===== Fold 5 =====\n",
            "Epoch 1: Train Loss=1.4347, Train Acc=0.4867, Val Acc=0.4399\n",
            "Epoch 2: Train Loss=1.0570, Train Acc=0.6263, Val Acc=0.5927\n",
            "Epoch 3: Train Loss=0.8725, Train Acc=0.6944, Val Acc=0.6511\n",
            "Epoch 4: Train Loss=0.7475, Train Acc=0.7379, Val Acc=0.6672\n",
            "Epoch 5: Train Loss=0.6390, Train Acc=0.7767, Val Acc=0.6590\n",
            "Epoch 6: Train Loss=0.5393, Train Acc=0.8098, Val Acc=0.7112\n",
            "Epoch 7: Train Loss=0.4569, Train Acc=0.8401, Val Acc=0.7172\n",
            "Epoch 8: Train Loss=0.3695, Train Acc=0.8699, Val Acc=0.7188\n",
            "Epoch 9: Train Loss=0.3029, Train Acc=0.8946, Val Acc=0.7030\n",
            "Epoch 10: Train Loss=0.2429, Train Acc=0.9147, Val Acc=0.7305\n",
            "\n",
            "Cross-validation results:\n",
            "Fold 1: Best Val Acc=0.7426, Runtime=136.54s\n",
            "Fold 2: Best Val Acc=0.7376, Runtime=135.56s\n",
            "Fold 3: Best Val Acc=0.7416, Runtime=135.40s\n",
            "Fold 4: Best Val Acc=0.7508, Runtime=135.63s\n",
            "Fold 5: Best Val Acc=0.7305, Runtime=134.60s\n"
          ]
        }
      ],
      "source": [
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_results = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_tensor, y_train_tensor)):\n",
        "    print(f\"\\n===== Fold {fold+1} =====\")\n",
        "\n",
        "    train_ds = TensorDataset(X_train_tensor[train_idx], y_train_tensor[train_idx])\n",
        "    val_ds   = TensorDataset(X_train_tensor[val_idx], y_train_tensor[val_idx])\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "    best_val_acc, runtime, _ = train_resnet(train_loader, val_loader, num_epochs=10)\n",
        "\n",
        "    cv_results.append((fold+1, best_val_acc, runtime))\n",
        "\n",
        "print(\"\\nCross-validation results:\")\n",
        "for fold, acc, rt in cv_results:\n",
        "    print(f\"Fold {fold}: Best Val Acc={acc:.4f}, Runtime={rt:.2f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final training time on full dataset: 241.61s\n"
          ]
        }
      ],
      "source": [
        "# Train on full dataset\n",
        "full_train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "full_train_loader = DataLoader(full_train_ds, batch_size=64, shuffle=True)\n",
        "\n",
        "_, train_time, final_model = train_resnet(full_train_loader, num_epochs=15)  # train longer\n",
        "\n",
        "print(f\"\\nFinal training time on full dataset: {train_time:.2f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.7354\n",
            "Inference time (avg ms/sample): 0.06238467000075616\n",
            "Confusion matrix:\n",
            " [[832   9  20  12  32   4  16   3  35  37]\n",
            " [ 18 772   6   3   3   2   8   3  29 156]\n",
            " [ 66   6 548  56 156  31  79  25  17  16]\n",
            " [ 25   6  67 492 138  94  82  42  24  30]\n",
            " [ 13   4  22  25 822   4  36  64   7   3]\n",
            " [ 12   6  49 171 113 493  50  75  11  20]\n",
            " [  5   3   9  45  69  16 834   4   6   9]\n",
            " [ 15   3  18  15  69  19   9 825   6  21]\n",
            " [ 68  16   5   5  10   3   8   0 859  26]\n",
            " [ 33  27   4   9   2   4   7  11  26 877]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.83      0.80      1000\n",
            "           1       0.91      0.77      0.83      1000\n",
            "           2       0.73      0.55      0.63      1000\n",
            "           3       0.59      0.49      0.54      1000\n",
            "           4       0.58      0.82      0.68      1000\n",
            "           5       0.74      0.49      0.59      1000\n",
            "           6       0.74      0.83      0.78      1000\n",
            "           7       0.78      0.82      0.80      1000\n",
            "           8       0.84      0.86      0.85      1000\n",
            "           9       0.73      0.88      0.80      1000\n",
            "\n",
            "    accuracy                           0.74     10000\n",
            "   macro avg       0.74      0.74      0.73     10000\n",
            "weighted avg       0.74      0.74      0.73     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Inference on test set\n",
        "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=64, shuffle=False)\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "final_model.eval()\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        preds = final_model(xb)\n",
        "        _, predicted = preds.max(1)\n",
        "        y_true.extend(yb.tolist())\n",
        "        y_pred.extend(predicted.cpu().tolist())\n",
        "t1 = time.perf_counter()\n",
        "\n",
        "inf_time_per_sample = (t1 - t0) / len(X_test_tensor)\n",
        "\n",
        "# Metrics\n",
        "print(\"Test accuracy:\", accuracy_score(y_true, y_pred))\n",
        "print(\"Inference time (avg ms/sample):\", inf_time_per_sample*1000)\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
        "print(classification_report(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear SVM Baseline\n",
        "\n",
        "**Setup:**\n",
        "- Input: Flattened CIFAR-10 images (32×32×3 → 3072 features).\n",
        "- Preprocessing: Standardization with `StandardScaler`.\n",
        "- Model: `LinearSVC` (only linear kernel, no RBF/poly).\n",
        "- Hyperparameter Search: GridSearchCV with `C ∈ {0.1, 0.01, 0.001}` (5-fold CV).\n",
        "- Training: 15 fits in total.\n",
        "\n",
        "**Results:**\n",
        "- Best Hyperparameter: `C = 0.001`\n",
        "- Cross-Validation Accuracy: ~0.394\n",
        "- Final Test Accuracy: **39.7%**\n",
        "- Inference Speed: ~0.035 ms/sample\n",
        "\n",
        "**Observations:**\n",
        "- Limited by linear decision boundaries → struggles with complex, non-linear image patterns.\n",
        "- Accuracy is only modestly above random guessing (10% for CIFAR-10).\n",
        "- Confusion matrix shows widespread misclassification across categories.\n",
        "\n",
        "## ResNet18 (CNN) Baseline\n",
        "\n",
        "**Setup:**\n",
        "- Input: CIFAR-10 images reshaped to `(N, 3, 32, 32)` and normalized to `[0,1]`.\n",
        "- Model: ResNet18 (transfer learning variant, adapted for 10 classes).\n",
        "- Training: 5-fold Stratified CV (10 epochs each fold).\n",
        "- Optimizer: Adam, LR = 0.001, Batch size = 64.\n",
        "- Final training on full training set with same configuration.\n",
        "\n",
        "**Results:**\n",
        "- Cross-Validation Accuracy (best per fold): ~73–75%\n",
        "- Final Test Accuracy: **73.5%**\n",
        "- Inference Speed: ~0.062 ms/sample\n",
        "- Confusion matrix shows much stronger diagonal dominance compared to SVM.\n",
        "\n",
        "**Observations:**\n",
        "- CNN clearly outperforms SVM, capturing local spatial patterns in images.\n",
        "- Some class confusion remains (e.g., between vehicles and animals).\n",
        "- Stronger generalization even with relatively short training (10 epochs).\n",
        "\n",
        "---\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "- **SVM Baseline:** Demonstrates the challenge of applying classical ML directly to raw image pixels. Good as a \"classical ML reference\" but not competitive.\n",
        "- **ResNet18 Baseline:** Achieves ~73% accuracy, showing the effectiveness of deep CNNs for image classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Trying something further, with hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A dictionary to store the results\n",
        "learning_rates = [0.0001, 0.001, 0.01]\n",
        "results = {}\n",
        "\n",
        "# We'll use the first fold from your 5-fold split for this experiment\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "train_idx, val_idx = next(iter(skf.split(X_train_tensor, y_train_tensor)))\n",
        "\n",
        "# Create the datasets and dataloaders\n",
        "train_ds = TensorDataset(X_train_tensor[train_idx], y_train_tensor[train_idx])\n",
        "val_ds = TensorDataset(X_train_tensor[val_idx], y_train_tensor[val_idx])\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Training with learning rate: 0.0001 ---\n",
            "Epoch 1: Train Loss=1.5729, Train Acc=0.4317, Val Acc=0.5115\n",
            "Epoch 2: Train Loss=1.1996, Train Acc=0.5707, Val Acc=0.5618\n",
            "Epoch 3: Train Loss=0.9855, Train Acc=0.6504, Val Acc=0.5921\n",
            "Epoch 4: Train Loss=0.8014, Train Acc=0.7165, Val Acc=0.5802\n",
            "Epoch 5: Train Loss=0.6312, Train Acc=0.7778, Val Acc=0.6056\n",
            "Epoch 6: Train Loss=0.4707, Train Acc=0.8366, Val Acc=0.6031\n",
            "Epoch 7: Train Loss=0.3409, Train Acc=0.8811, Val Acc=0.6038\n",
            "Epoch 8: Train Loss=0.2621, Train Acc=0.9082, Val Acc=0.5948\n",
            "Epoch 9: Train Loss=0.2140, Train Acc=0.9250, Val Acc=0.6198\n",
            "Epoch 10: Train Loss=0.1728, Train Acc=0.9405, Val Acc=0.6072\n",
            "Epoch 11: Train Loss=0.1604, Train Acc=0.9445, Val Acc=0.6119\n",
            "Epoch 12: Train Loss=0.1403, Train Acc=0.9510, Val Acc=0.6154\n",
            "Epoch 13: Train Loss=0.1325, Train Acc=0.9529, Val Acc=0.6132\n",
            "Epoch 14: Train Loss=0.1181, Train Acc=0.9589, Val Acc=0.6132\n",
            "Epoch 15: Train Loss=0.1099, Train Acc=0.9623, Val Acc=0.5979\n",
            "--- Training with learning rate: 0.001 ---\n",
            "Epoch 1: Train Loss=1.4480, Train Acc=0.4781, Val Acc=0.5256\n",
            "Epoch 2: Train Loss=1.0532, Train Acc=0.6297, Val Acc=0.6276\n",
            "Epoch 3: Train Loss=0.8738, Train Acc=0.6929, Val Acc=0.6180\n",
            "Epoch 4: Train Loss=0.7396, Train Acc=0.7421, Val Acc=0.6685\n",
            "Epoch 5: Train Loss=0.6397, Train Acc=0.7761, Val Acc=0.6500\n",
            "Epoch 6: Train Loss=0.5440, Train Acc=0.8105, Val Acc=0.6833\n",
            "Epoch 7: Train Loss=0.4555, Train Acc=0.8418, Val Acc=0.7439\n",
            "Epoch 8: Train Loss=0.3699, Train Acc=0.8711, Val Acc=0.6526\n",
            "Epoch 9: Train Loss=0.2984, Train Acc=0.8973, Val Acc=0.7362\n",
            "Epoch 10: Train Loss=0.2461, Train Acc=0.9140, Val Acc=0.7247\n",
            "Epoch 11: Train Loss=0.2014, Train Acc=0.9304, Val Acc=0.7208\n",
            "Epoch 12: Train Loss=0.1689, Train Acc=0.9404, Val Acc=0.7249\n",
            "Epoch 13: Train Loss=0.1498, Train Acc=0.9475, Val Acc=0.7441\n",
            "Epoch 14: Train Loss=0.1249, Train Acc=0.9572, Val Acc=0.7311\n",
            "Epoch 15: Train Loss=0.1125, Train Acc=0.9607, Val Acc=0.7210\n",
            "--- Training with learning rate: 0.01 ---\n",
            "Epoch 1: Train Loss=1.7550, Train Acc=0.3735, Val Acc=0.4376\n",
            "Epoch 2: Train Loss=1.2926, Train Acc=0.5329, Val Acc=0.4377\n",
            "Epoch 3: Train Loss=1.0625, Train Acc=0.6239, Val Acc=0.5993\n",
            "Epoch 4: Train Loss=0.9109, Train Acc=0.6812, Val Acc=0.6286\n",
            "Epoch 5: Train Loss=0.7901, Train Acc=0.7258, Val Acc=0.6904\n",
            "Epoch 6: Train Loss=0.6782, Train Acc=0.7640, Val Acc=0.7214\n",
            "Epoch 7: Train Loss=0.5934, Train Acc=0.7939, Val Acc=0.6875\n",
            "Epoch 8: Train Loss=0.5081, Train Acc=0.8239, Val Acc=0.6542\n",
            "Epoch 9: Train Loss=0.4251, Train Acc=0.8515, Val Acc=0.7179\n",
            "Epoch 10: Train Loss=0.3544, Train Acc=0.8766, Val Acc=0.7173\n",
            "Epoch 11: Train Loss=0.2987, Train Acc=0.8949, Val Acc=0.7122\n",
            "Epoch 12: Train Loss=0.2368, Train Acc=0.9174, Val Acc=0.7099\n",
            "Epoch 13: Train Loss=0.2019, Train Acc=0.9291, Val Acc=0.7192\n",
            "Epoch 14: Train Loss=0.1799, Train Acc=0.9377, Val Acc=0.6947\n",
            "Epoch 15: Train Loss=0.1595, Train Acc=0.9440, Val Acc=0.7297\n"
          ]
        }
      ],
      "source": [
        "for lr in learning_rates:\n",
        "    print(f\"--- Training with learning rate: {lr} ---\")\n",
        "    \n",
        "    # We only need the accuracy, so we can use _ to ignore the other return values\n",
        "    best_val_acc, _, _ = train_resnet(train_loader, val_loader, num_epochs=15, lr=lr)\n",
        "    \n",
        "    results[lr] = best_val_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Final Tuning Results ---\n",
            "{0.0001: 0.6198, 0.001: 0.7441, 0.01: 0.7297}\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Final Tuning Results ---\")\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on above experimentation we dont get much improvement by just working with different lr values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "lets try with data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CIFAR10Dataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        \n",
        "        # PyTorch transforms expect a PIL Image, so we convert the numpy array\n",
        "        image = transforms.ToPILImage()(image)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return image, label\n",
        "    \n",
        "    \n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Simpler pipeline for test data (no random augmentation)\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# --- 2. Create Datasets and DataLoaders ---\n",
        "# Use the custom Dataset class to apply the transforms\n",
        "# Remember X_train_img is the (N, H, W, C) numpy array from earlier\n",
        "train_dataset = CIFAR10Dataset(images=X_train_img, labels=y_train, transform=train_transforms)\n",
        "val_dataset = CIFAR10Dataset(images=X_train_img, labels=y_train, transform=val_transforms) # Use test transforms for validation\n",
        "\n",
        "# Get a validation split\n",
        "train_size = int(0.9 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We need to make sure the validation subset uses the simple test_transforms\n",
        "val_subset.dataset = val_dataset\n",
        "\n",
        "# Create the DataLoaders\n",
        "exp1_train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "exp1_val_loader = DataLoader(val_subset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Running Experiment 1: Augmentation Only ---\n",
            "Epoch 1: Train Loss=1.4616, Train Acc=0.4725, Val Acc=0.5186\n",
            "Epoch 2: Train Loss=1.1124, Train Acc=0.6061, Val Acc=0.5870\n",
            "Epoch 3: Train Loss=0.9536, Train Acc=0.6654, Val Acc=0.6182\n",
            "Epoch 4: Train Loss=0.8633, Train Acc=0.7012, Val Acc=0.6918\n",
            "Epoch 5: Train Loss=0.7867, Train Acc=0.7263, Val Acc=0.6866\n",
            "Epoch 6: Train Loss=0.7309, Train Acc=0.7467, Val Acc=0.6860\n",
            "Epoch 7: Train Loss=0.6758, Train Acc=0.7652, Val Acc=0.7456\n",
            "Epoch 8: Train Loss=0.6398, Train Acc=0.7788, Val Acc=0.7458\n",
            "Epoch 9: Train Loss=0.6007, Train Acc=0.7914, Val Acc=0.7640\n",
            "Epoch 10: Train Loss=0.5640, Train Acc=0.8061, Val Acc=0.7156\n",
            "Epoch 11: Train Loss=0.5284, Train Acc=0.8162, Val Acc=0.7658\n",
            "Epoch 12: Train Loss=0.4997, Train Acc=0.8263, Val Acc=0.7852\n",
            "Epoch 13: Train Loss=0.4676, Train Acc=0.8349, Val Acc=0.7750\n",
            "Epoch 14: Train Loss=0.4442, Train Acc=0.8442, Val Acc=0.7740\n",
            "Epoch 15: Train Loss=0.4247, Train Acc=0.8502, Val Acc=0.7394\n",
            "\n",
            "--- Experiment 1 Complete ---\n",
            "Best Validation Accuracy with Augmentation: 0.7852\n"
          ]
        }
      ],
      "source": [
        "print(\"--- Running Experiment 1: Augmentation Only ---\")\n",
        "# We use the ORIGINAL train_resnet function from cell [26]\n",
        "best_val_acc_exp1, runtime_exp1, model_exp1 = train_resnet(exp1_train_loader, exp1_val_loader, num_epochs=15)\n",
        "\n",
        "print(f\"\\n--- Experiment 1 Complete ---\")\n",
        "print(f\"Best Validation Accuracy with Augmentation: {best_val_acc_exp1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "experiment 2 with LR scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "def train_resnet_with_scheduler(train_loader, val_loader, num_epochs=15, lr=0.001):\n",
        "    \"\"\"\n",
        "    A training function that uses a ReduceLROnPlateau scheduler.\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    \n",
        "    # Using the original model with weights=None\n",
        "    model = resnet18(weights=None)\n",
        "    model.fc = nn.Linear(model.fc.in_features, 10)\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    \n",
        "    # --- CHANGE 1: Create the scheduler ---\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
        "\n",
        "    best_val_acc = 0\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        # Training loop for one epoch (as in the original function)\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # --- CHANGE 2: Calculate validation loss and step the scheduler ---\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                preds = model(xb)\n",
        "                loss = criterion(preds, yb)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = preds.max(1)\n",
        "                total += yb.size(0)\n",
        "                correct += (predicted == yb).sum().item()\n",
        "        \n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_acc = correct / total\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}: Val Loss={avg_val_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
        "        \n",
        "        # Pass the validation loss to the scheduler\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    total_time = time.perf_counter() - start_time\n",
        "        \n",
        "    # We can return the final validation accuracy to compare experiments\n",
        "    return best_val_acc, total_time, best_model_state\n",
        "\n",
        "# def train_resnet(train_loader, val_loader=None, num_epochs=10, lr=0.001):\n",
        "#     model = resnet18(weights=None)\n",
        "#     model.fc = nn.Linear(model.fc.in_features, 10)\n",
        "#     model = model.to(device)\n",
        "\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "#     optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "#     best_val_acc = 0\n",
        "#     start_time = time.perf_counter()\n",
        "\n",
        "#     for epoch in range(num_epochs):\n",
        "#         model.train()\n",
        "#         running_loss, correct, total = 0, 0, 0\n",
        "\n",
        "#         for xb, yb in train_loader:\n",
        "#             xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "#             optimizer.zero_grad()\n",
        "#             preds = model(xb)\n",
        "#             loss = criterion(preds, yb)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             running_loss += loss.item()\n",
        "#             _, predicted = preds.max(1)\n",
        "#             total += yb.size(0)\n",
        "#             correct += (predicted == yb).sum().item()\n",
        "\n",
        "#         train_acc = correct / total\n",
        "\n",
        "#         if val_loader:\n",
        "#             model.eval()\n",
        "#             correct, total = 0, 0\n",
        "#             with torch.no_grad():\n",
        "#                 for xb, yb in val_loader:\n",
        "#                     xb, yb = xb.to(device), yb.to(device)\n",
        "#                     preds = model(xb)\n",
        "#                     _, predicted = preds.max(1)\n",
        "#                     total += yb.size(0)\n",
        "#                     correct += (predicted == yb).sum().item()\n",
        "#             val_acc = correct / total\n",
        "#             print(f\"Epoch {epoch+1}: \"\n",
        "#                   f\"Train Loss={running_loss/len(train_loader):.4f}, \"\n",
        "#                   f\"Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}\")\n",
        "#             best_val_acc = max(best_val_acc, val_acc)\n",
        "\n",
        "#     total_time = time.perf_counter() - start_time\n",
        "#     return best_val_acc, total_time, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Running Experiment 2: Augmentation + Scheduler ---\n",
            "Epoch 1/15: Val Loss=1.2329, Val Acc=0.5596\n",
            "Epoch 2/15: Val Loss=1.1830, Val Acc=0.5890\n",
            "Epoch 3/15: Val Loss=0.9177, Val Acc=0.6762\n",
            "Epoch 4/15: Val Loss=1.0434, Val Acc=0.6482\n",
            "Epoch 5/15: Val Loss=0.8814, Val Acc=0.6960\n",
            "Epoch 6/15: Val Loss=0.7438, Val Acc=0.7406\n",
            "Epoch 7/15: Val Loss=0.9692, Val Acc=0.6736\n",
            "Epoch 8/15: Val Loss=0.7487, Val Acc=0.7438\n",
            "Epoch 9/15: Val Loss=0.9534, Val Acc=0.6896\n",
            "Epoch 10/15: Val Loss=0.7217, Val Acc=0.7582\n",
            "Epoch 11/15: Val Loss=0.7049, Val Acc=0.7702\n",
            "Epoch 12/15: Val Loss=0.6746, Val Acc=0.7736\n",
            "Epoch 13/15: Val Loss=0.7880, Val Acc=0.7414\n",
            "Epoch 14/15: Val Loss=0.6298, Val Acc=0.7922\n",
            "Epoch 15/15: Val Loss=0.6374, Val Acc=0.7906\n",
            "Best Validation Accuracy with Augmentation + Scheduler: 0.7922\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Running Experiment 2: Augmentation + Scheduler ---\")\n",
        "best_val_acc_exp2, runtime_exp2, model_exp2 = train_resnet_with_scheduler(exp1_train_loader, exp1_val_loader, num_epochs=15)\n",
        "print(f\"Best Validation Accuracy with Augmentation + Scheduler: {best_val_acc_exp2:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "229.06875680001394\n"
          ]
        }
      ],
      "source": [
        "print(runtime_exp2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets try using pre trained ResNet weights instead of training from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "transfer_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# --- 2. Create Datasets and DataLoaders ---\n",
        "# NOTE: We are NOT using random augmentation for this experiment\n",
        "# The goal is to isolate the effect of transfer learning.\n",
        "exp3_dataset = CIFAR10Dataset(images=X_train_img, labels=y_train, transform=transfer_transforms)\n",
        "\n",
        "train_size = int(0.9 * len(exp3_dataset))\n",
        "val_size = len(exp3_dataset) - train_size\n",
        "train_subset, val_subset = torch.utils.data.random_split(exp3_dataset, [train_size, val_size])\n",
        "\n",
        "exp3_train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "exp3_val_loader = DataLoader(val_subset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_resnet_transfer(train_loader, val_loader, num_epochs=15, lr=0.001):\n",
        "    \"\"\"\n",
        "    A simple training function that uses a pre-trained model.\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    \n",
        "    # --- The key change is here ---\n",
        "    model = resnet18(weights='IMAGENET1K_V1')\n",
        "    model.fc = nn.Linear(model.fc.in_features, 10)\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    \n",
        "    best_val_acc = 0\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                preds = model(xb)\n",
        "                loss = criterion(preds, yb)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = preds.max(1)\n",
        "                total += yb.size(0)\n",
        "                correct += (predicted == yb).sum().item()\n",
        "        val_acc = correct / total\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        \n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}: Val Loss={avg_val_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
        "\n",
        "    total_time = time.perf_counter() - start_time\n",
        "    return best_val_acc, total_time, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Running Experiment 3: Transfer Learning Only ---\n",
            "Epoch 1/15: Val Loss=0.8554, Val Acc=0.7142\n",
            "Epoch 2/15: Val Loss=0.8625, Val Acc=0.7378\n",
            "Epoch 3/15: Val Loss=0.7418, Val Acc=0.7580\n",
            "Epoch 4/15: Val Loss=0.6148, Val Acc=0.7904\n",
            "Epoch 5/15: Val Loss=0.5853, Val Acc=0.8034\n",
            "Epoch 6/15: Val Loss=0.6332, Val Acc=0.8046\n",
            "Epoch 7/15: Val Loss=0.6428, Val Acc=0.8142\n",
            "Epoch 8/15: Val Loss=0.6788, Val Acc=0.8092\n",
            "Epoch 9/15: Val Loss=0.7252, Val Acc=0.8002\n",
            "Epoch 10/15: Val Loss=0.7304, Val Acc=0.8074\n",
            "Epoch 11/15: Val Loss=0.8368, Val Acc=0.7972\n",
            "Epoch 12/15: Val Loss=0.7034, Val Acc=0.8122\n",
            "Epoch 13/15: Val Loss=0.7973, Val Acc=0.8028\n",
            "Epoch 14/15: Val Loss=0.8783, Val Acc=0.7972\n",
            "Epoch 15/15: Val Loss=0.8682, Val Acc=0.8022\n",
            "Best Validation Accuracy with Transfer Learning: 0.8142\n",
            "230.4384614000155\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Running Experiment 3: Transfer Learning Only ---\")\n",
        "best_val_acc_exp3, total_time_exp3, model_exp3  = train_resnet_transfer(exp3_train_loader, exp3_val_loader, num_epochs=15)\n",
        "print(f\"Best Validation Accuracy with Transfer Learning: {best_val_acc_exp3:.4f}\")\n",
        "print(total_time_exp3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One final experiment combining it all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_train_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "final_val_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_resnet_final(train_loader, val_loader, num_epochs=15, lr=0.001):\n",
        "    \"\"\"\n",
        "    The final, fully-upgraded training function combining all techniques.\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    \n",
        "    # 1. Transfer Learning\n",
        "    model = resnet18(weights='IMAGENET1K_V1')\n",
        "    model.fc = nn.Linear(model.fc.in_features, 10)\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    \n",
        "    # 2. Learning Rate Scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                preds = model(xb)\n",
        "                loss = criterion(preds, yb)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = preds.max(1)\n",
        "                total += yb.size(0)\n",
        "                correct += (predicted == yb).sum().item()\n",
        "        \n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_acc = correct / total\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}: Val Loss={avg_val_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
        "        \n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            \n",
        "    total_time = time.perf_counter() - start_time\n",
        "    \n",
        "    return best_val_acc, total_time, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_train_dataset = CIFAR10Dataset(images=X_train_img, labels=y_train, transform=final_train_transforms)\n",
        "final_val_dataset = CIFAR10Dataset(images=X_train_img, labels=y_train, transform=final_val_transforms)\n",
        "\n",
        "train_size = int(0.9 * len(final_train_dataset))\n",
        "val_size = len(final_train_dataset) - train_size\n",
        "train_subset, val_subset = torch.utils.data.random_split(final_train_dataset, [train_size, val_size])\n",
        "val_subset.dataset = final_val_dataset\n",
        "\n",
        "final_train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "final_val_loader = DataLoader(val_subset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Running Experiment 4: All Together ---\n",
            "Epoch 1/15: Val Loss=0.8431, Val Acc=0.7158\n",
            "Epoch 2/15: Val Loss=0.6477, Val Acc=0.7768\n",
            "Epoch 3/15: Val Loss=0.6378, Val Acc=0.7784\n",
            "Epoch 4/15: Val Loss=0.6120, Val Acc=0.7902\n",
            "Epoch 5/15: Val Loss=0.5791, Val Acc=0.8026\n",
            "Epoch 6/15: Val Loss=0.5613, Val Acc=0.8082\n",
            "Epoch 7/15: Val Loss=0.8601, Val Acc=0.7836\n",
            "Epoch 8/15: Val Loss=0.5480, Val Acc=0.8124\n",
            "Epoch 9/15: Val Loss=0.5160, Val Acc=0.8286\n",
            "Epoch 10/15: Val Loss=0.4931, Val Acc=0.8318\n",
            "Epoch 11/15: Val Loss=0.5068, Val Acc=0.8294\n",
            "Epoch 12/15: Val Loss=0.5238, Val Acc=0.8304\n",
            "Epoch 13/15: Val Loss=0.5285, Val Acc=0.8232\n",
            "Epoch 14/15: Val Loss=0.7846, Val Acc=0.7868\n",
            "Epoch 15/15: Val Loss=0.5302, Val Acc=0.8406\n",
            "Best Validation Accuracy with All Techniques: 0.8406\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Running Experiment 4: All Together ---\")\n",
        "best_val_acc_exp4, final_runtime, final_model = train_resnet_final(final_train_loader, final_val_loader, num_epochs=15)\n",
        "print(f\"Best Validation Accuracy with All Techniques: {best_val_acc_exp4:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
